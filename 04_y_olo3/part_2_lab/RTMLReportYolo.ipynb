{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "# RTML LAB Report \n",
    "## Lab 4.2 YOLO training\n",
    "\n",
    "----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Name = 'ati tesakulsiri'\n",
    "ID = 'st123009'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1 Introduction\n",
    "\n",
    "- #### Object detection\n",
    "The concept of object detection in computer vision includes identifying different things in digital photos or movies. Among the things found are people, vehicles, chairs, stones, structures, and animals.\n",
    "\n",
    "- #### YOLO\n",
    "YOLO is a method that provides real-time object detection using neural networks. The popularity of this algorithm is due to its accuracy and quickness. It has been applied in a variety of ways to identify animals, humans, parking meters, and traffic lights.\n",
    "\n",
    "The YOLO method for object detection is described in this article along with its workings. It also highlights a few of its practical uses.\n",
    "\n",
    "- #### Yolov4\n",
    "- YOLO v4 was developed based on YOLO v3 by a new group of authors, Alexey Bochkovskiy and colleagues, who took\n",
    "over the development of Darknet and YOLO after [Joseph Redmon quit computer vision research](https://twitter.com/pjreddie/status/1230524770350817280?lang=en).\n",
    "- Take a look at the [YOLO v4 paper](https://arxiv.org/abs/2004.10934). The authors make many small and some large\n",
    "improvements to YOLOv3 to achieve a higher frame rate and higher accuracy. Source code is available at the\n",
    "[Darknet GitHub repository](https://github.com/AlexeyAB/darknet).\n",
    "    - Bag of spacial \n",
    "\n",
    "        - Mish activation Function\n",
    "\n",
    "\n",
    "            - Next, let's take a look at the newish activation function used in YOLOv4: Mish.\n",
    "        Mish is a SoftPlus activation function that is non-monotonic and designed for\n",
    "        neural networks that regularize themselves. It was inspired by the *swish* activation function.\n",
    "        It has a range from -0.31 to $\\infty$, due to the SoftPlus function:\n",
    "\n",
    "$$\\mathrm{SoftPlus}(x)=\\ln(1+e^x) \\\\\n",
    "f(x)=x \\tanh(\\mathrm{SoftPlus}(x))=x \\tanh(\\ln(1+e^x)) $$.\n",
    "\n",
    "<img src = '/root/keep_lab/RTML_Labsession/04_y_olo3/to_submit/mish_activation_function_graph.png' title=\"weight\" style=\"width: 480px;\" />\n",
    "\n",
    "\n",
    "- ### Mean Average precision\n",
    "Mean Average Precision(mAP) is a metric used to evaluate object detection models such as Fast R-CNN, YOLO, Mask R-CNN, etc. The mean of average precision(AP) values are calculated over recall values from 0 to 1.\n",
    "- mAP formula is based on the following sub metrics:\n",
    "    - Confusion Matrix,\n",
    "    - Intersection over Union(IoU),\n",
    "    - Recall, \n",
    "    - Precision\n",
    "\n",
    "\n",
    "To create a confusion matrix, we need four attributes:\n",
    "\n",
    "True Positives (TP):  The model predicted a label and matches correctly as per ground truth.\n",
    "\n",
    "True Negatives (TN): The model does not predict the label and is not a part of the ground truth.\n",
    "\n",
    "False Positives (FP): The model predicted a label, but it is not a part of the ground truth (Type I Error).\n",
    "\n",
    "False Negatives (FN): The model does not predict a label, but it is part of the ground truth. (Type II Error).\n",
    "\n",
    "**Intersection over Union** indicates the overlap of the predicted bounding box coordinates to the ground truth box. Higher IoU indicates the predicted bounding box coordinates closely resembles the ground truth box coordinates.\n",
    "\n",
    "**Precision**\n",
    "Precision measures how well you can find true positives(TP) out of all positive predictions. (TP+FP).\n",
    "\n",
    "Average Precision is calculated as the weighted mean of precisions at each threshold; the weight is the increase in recall from the prior threshold.\n",
    "\n",
    "**Mean Average Precision** is the average of AP of each class. However, the interpretation of AP and mAP varies in different contexts. For instance, in the evaluation document of the COCO object detection challenge, AP and mAP are the same.\n",
    "\n",
    "Here is a summary of the steps to calculate the AP:\n",
    "\n",
    "Generate the prediction scores using the model.\n",
    "\n",
    "Convert the prediction scores to class labels.\n",
    "\n",
    "Calculate the confusion matrixâ€”TP, FP, TN, FN.\n",
    "\n",
    "Calculate the precision and recall metrics.\n",
    "\n",
    "Calculate the area under the precision-recall curve.\n",
    "\n",
    "Measure the average precision.\n",
    "\n",
    "The mAP is calculated by finding Average Precision(AP) for each class and then average over a number of classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Complete IoU Loss (CIoU loss)\n",
    "CIoU loss bounding box regression uses three geometric factors.\n",
    "\n",
    "Overlap area between the predicted box and the ground truth bounding box-IOU loss\n",
    "\n",
    "The central point between the predicted box and the ground truth bounding box-DIoU loss\n",
    "\n",
    "An aspect ratio of the predicted box and the ground truth box\n",
    "\n",
    "As CIoU loss uses complete geometric factors, it converges faster than GIoU loss. It improves average precision (AP) and average recall (AR) for object detection and segmentation.\n",
    "\n",
    "CIoU loss is an aggregation of the overlap area, distance, and aspect ratio, respectively, referred to as Complete IOU loss.\n",
    "\n",
    "S is the overlap area denoted by S=1-IoU\n",
    "\n",
    "D is the normalized distance Iou loss between the center point of the predicted and ground truth boxes.\n",
    "\n",
    "V is the consistency of the aspect ratio.\n",
    "\n",
    "All S, V, and D are invariant to the regression scale and are normalized to values between 0 and 1.\n",
    "\n",
    "CIoU loss, like GIoU loss and DIoU loss, moves the predicted bounding box towards the ground truth bounding box for non-overlapping cases.\n",
    "\n",
    "CIoU loss needs fewer iterations to converges than GIoU loss. CIoU loss makes regression very fast with extreme aspect ratios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Method\n",
    "- We will Continue from Last lab which is \n",
    "   1. Implementation of the mish activation function\n",
    "   2. Option for the maxpool layer in the `create_modules` function and in your model's `forward()` method.\n",
    "   3. Enabling a `[route]` module to concatenate more than two previous layers\n",
    "   4. Loading the pre-trained weights [provided by the authors](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights)\n",
    "   4. Scale inputs to 608,608 and make sure you're passing input channels in RGB order, not OpenCV's BGR order.\n",
    "   <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "- Now, we are going to implement.\n",
    "   Train the YOLOv4 model on the COCO dataset (or another dataset if you have one available).\n",
    "   Here the purpose is not to get the best possible model (that would require implementing all\n",
    "   of the \"bag of freebies\" training tricks described in the paper), but just some of them, to\n",
    "   get a feel for their importance.\n",
    "\n",
    "1. Get a set of ImageNet pretrained weights for CSPDarknet53\n",
    "\n",
    "2. Load the pretrained weights into the backbone portion of PyTorch YOLOv4 model.\n",
    "\n",
    "3. Implement a function similar to train model developed in previous labs for classifiers that preprocesses the input with       basic augmentation transformations, converts the\n",
    "   anchor-relative outputs to bounding box coordinates, computes MSE loss for the bounding box coordinates,\n",
    "   backpropagates the loss, and takes a step for the optimizer. Use the recommended IoU thresholds to determine\n",
    "   which predicted bounding boxes to include in the loss. You will find many examples of how to do this\n",
    "   online.\n",
    "\n",
    "4. Train model on COCO. \n",
    "\n",
    "5. Compute mAP for model on the COCO validation set.\n",
    "\n",
    "6. Implement the CIoU loss function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 result\n",
    "\n",
    "- ### 1,2 CSPDarknet53 Pre-trained for backbone\n",
    "- For download part\n",
    "   - `-rw-r--r-- 1 root 5097 110710068 Feb 10 04:16 csdarknet53-omega_final.weights`\n",
    "   - `-rw-r--r-- 1 root 5097 248007048 Feb 10 04:16 yolov3.weights`\n",
    "   - `-rw-r--r-- 1 root 5097 257717640 Feb 10 04:16 yolov4.weights`\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- For adding Partial portion of the model( backbone) of darknet we re-implement the `load-weights` method in DarkNets Class\n",
    "> Note that I skip the code to shorten the report by putting the code to be `...`\n",
    "```python\n",
    "   def load_weights_(self, weightfile, backbone=False):\n",
    "       ...\n",
    "\n",
    "        stage = 1\n",
    "    \n",
    "        if(backbone):\n",
    "            stage = 0\n",
    "\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type = self.blocks[i + 1][\"type\"]\n",
    "    \n",
    "            #If module_type is convolutional load weights\n",
    "            #Otherwise ignore.\n",
    "            if(backbone):\n",
    "                # print(stage, self.blocks[i + 1])\n",
    "                if(stage == 2): break\n",
    "\n",
    "                if(\"backbone\" in self.blocks[i + 1] and int(self.blocks[i + 1][\"backbone\"]) == 0):\n",
    "                    stage = 1\n",
    "                elif(\"backbone\" in self.blocks[i + 1] and int(self.blocks[i + 1][\"backbone\"]) == 1):\n",
    "                    stage = 2\n",
    "\n",
    "                if(stage == 0): continue\n",
    "            \n",
    "            # print(self.blocks[i + 1])\n",
    "            # Load weight\n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "                  \n",
    "                  ...\n",
    "\n",
    "                conv.weight.data.copy_(conv_weights)\n",
    "```\n",
    "\n",
    "\n",
    "- and now we load the `weight with new method`\n",
    "```python\n",
    "model = Darknet(\"cfg/yolov4.cfg\")\n",
    "# Edit Convo Layer 114\n",
    "model.module_list[114].conv_114 = nn.Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "model.load_weights_(\"csdarknet53-omega_final.weights\",True)\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = 608\n",
    "\n",
    "```\n",
    "\n",
    "-  Here are the result\n",
    "```bash\n",
    "Loading network.....\n",
    "Network successfully loaded\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 3 Implement the train Function\n",
    "    - The function are tweak from `lab direction`\n",
    "```py\n",
    "def run_training(model, optimizer, dataloader, device, img_size, n_epoch, every_n_batch, every_n_epoch, ckpt_dir):\n",
    "    losses = None\n",
    "    for epoch_i in range(n_epoch):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels, bboxes in dataloader:\n",
    "            inputs = Variable(torch.from_numpy(np.array(inputs)).squeeze(1).permute(0,3,1,2).float(),requires_grad=True)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = Variable(torch.stack(labels),requires_grad=True)\n",
    "            labels = labels.to(device)\n",
    "            #print(inputs.shape)\n",
    "\n",
    "            running_corrects = 0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs, True)\n",
    "\n",
    "                pred_xywh = outputs[..., 0:4] / img_size\n",
    "                pred_conf = outputs[..., 4:5]\n",
    "                pred_cls = outputs[..., 5:]\n",
    "\n",
    "                label_xywh = labels[..., :4] / img_size\n",
    "\n",
    "                label_obj_mask = labels[..., 4:5]\n",
    "                label_noobj_mask = (1.0 - label_obj_mask)\n",
    "                lambda_coord = 0.001\n",
    "                lambda_noobj = 0.05\n",
    "                label_cls = labels[..., 5:]\n",
    "                loss = nn.MSELoss()\n",
    "                loss_bce = nn.BCELoss()\n",
    "\n",
    "                loss_coord = lambda_coord * label_obj_mask * loss(input=pred_xywh, target=label_xywh)\n",
    "                loss_conf = (label_obj_mask * loss_bce(input=pred_conf, target=label_obj_mask)) + \\\n",
    "                            (lambda_noobj * label_noobj_mask * loss_bce(input=pred_conf, target=label_obj_mask))\n",
    "                loss_cls = label_obj_mask * loss_bce(input=pred_cls, target=label_cls)\n",
    "\n",
    "                loss_coord = torch.sum(loss_coord)\n",
    "                loss_conf = torch.sum(loss_conf)\n",
    "                loss_cls = torch.sum(loss_cls)\n",
    "\n",
    "                # print(pred_xywh.shape, label_xywh.shape)\n",
    "\n",
    "                ciou = CIOU_xywh_torch(pred_xywh, label_xywh)\n",
    "                # print(ciou.shape)\n",
    "                ciou = ciou.unsqueeze(-1)\n",
    "                # print(ciou.shape)\n",
    "                # print(label_obj_mask.shape)\n",
    "                loss_ciou = torch.sum(label_obj_mask * (1.0 - ciou))\n",
    "                # print(loss_coord)\n",
    "                loss =  loss_ciou +  loss_conf + loss_cls\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # print('Running loss')\n",
    "                # print(loss_coord, loss_conf, loss_cls)\n",
    "        epoch_loss = running_loss / 750\n",
    "        print(epoch_loss)\n",
    "        print('End Epoch')\n",
    "```\n",
    "- Here are the result\n",
    "``` bash\n",
    "index created!\n",
    "31.06427083333333\n",
    "End Epoch\n",
    "30.9518505859375\n",
    "End Epoch\n",
    "31.288087565104167\n",
    "End Epoch\n",
    "30.979817708333332\n",
    "End Epoch\n",
    "30.667805989583332\n",
    "End Epoch\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## 4. Train on COCO\n",
    "    - The custom COCO class are here\n",
    "```python\n",
    "class CustomCoco(CocoDetection):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            annFile: str,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            transforms: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super(CocoDetection, self).__init__(root, transforms, transform, target_transform)\n",
    "        from pycocotools.coco import COCO\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "        # self.target = target\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        img = np.array(img)\n",
    "\n",
    "        category_ids = list(obj['category_id'] for obj in target)\n",
    "        bboxes = list(obj['bbox'] for obj in target)\n",
    "  \n",
    "        if self.transform is not None:\n",
    "            bboxes = list(obj['bbox'] for obj in target)\n",
    "            category_ids = list(obj['category_id'] for obj in target)\n",
    "            transformed = self.transform(image=img, bboxes=bboxes, category_ids=category_ids)\n",
    "            img = transformed['image'],\n",
    "            bboxes = torch.Tensor(transformed['bboxes'])\n",
    "            cat_ids = torch.Tensor(transformed['category_ids'])\n",
    "            labels, bboxes = self.__create_label(bboxes, cat_ids.type(torch.IntTensor))\n",
    "\n",
    "        return img, labels, bboxes\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __create_label(self, bboxes, class_inds):\n",
    "\n",
    "\n",
    "        # print(\"Class indices: \", class_inds)\n",
    "        bboxes = np.array(bboxes)\n",
    "        class_inds = np.array(class_inds)\n",
    "        anchors = ANCHORS # all the anchors\n",
    "        strides = np.array(STRIDES) # list of strides\n",
    "        train_output_size = IP_SIZE / strides # image with different scales\n",
    "        anchors_per_scale = NUM_ANCHORS # anchor per scale\n",
    "        \n",
    "        ...\n",
    "        # print(train_output_size)\n",
    "\n",
    "                    \n",
    "```\n",
    "\n",
    "\n",
    "- Here are the result of loadong\n",
    "``` bash\n",
    "Load Dataset\n",
    "loading annotations into memory...\n",
    "Done (t=17.65s)\n",
    "creating index...\n",
    "index created!\n",
    "loading annotations into memory...\n",
    "Done (t=7.63s)\n",
    "creating index...\n",
    "```\n",
    "\n",
    "- After Training with `run_traning` function\n",
    "```bash\n",
    "31.06427083333333\n",
    "End Epoch\n",
    "30.9518505859375\n",
    "End Epoch\n",
    "31.288087565104167\n",
    "End Epoch\n",
    "30.979817708333332\n",
    "End Epoch\n",
    "30.667805989583332\n",
    "End Epoch\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 5. Calculate the mAPs of the model\n",
    "- I try to use the sklearn to calculate FN,TN,TP and FP and use the IoU function from the lab direcction file.\n",
    "    - Unfortunatly, we did not train the model long enough to predict the correct result.\n",
    "the partial of our result look like\n",
    "\n",
    "```bash\n",
    "n person person person person person person person person person person person person person person person person person person person person person pperson person person person person cat cat cat cat cat cat cat cat cat cat cat cla umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella rella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella umbrella skis skis skis skis skis skis skis skis skis skis skis skis skis baseball bat baseball bat baseball bat baseball bat baseball bat baseball bat baseball bat baseball bat baseball bat baseball bat baseball bat diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningtable diningier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair drier hair dr\n",
    "\n",
    "----------------------------------------------------------\n",
    "SUMMARY\n",
    "----------------------------------------------------------\n",
    "Task                     : Time Taken (in seconds)\n",
    "\n",
    "Reading addresses        : 0.000\n",
    "Loading batch            : 0.715\n",
    "Detection (1 images)     : 2.929\n",
    "Output Processing        : 0.000\n",
    "Drawing Boxes            : 0.889\n",
    "Average time_per_img     : 4.532\n",
    "----------------------------------------------------------\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 6.CIOU\n",
    "\n",
    "I have implement the code(from lab direction) below here.\n",
    "``` python\n",
    "def CIOU_xywh_torch(boxes1,boxes2):\n",
    "    '''\n",
    "    cal CIOU of two boxes or batch boxes\n",
    "    :param boxes1:[xmin,ymin,xmax,ymax] or\n",
    "                [[xmin,ymin,xmax,ymax],[xmin,ymin,xmax,ymax],...]\n",
    "    :param boxes2:[xmin,ymin,xmax,ymax]\n",
    "    :return:\n",
    "    '''\n",
    "    # cx cy w h->xyxy\n",
    "    boxes1 = torch.cat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
    "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], dim=-1)\n",
    "    boxes2 = torch.cat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
    "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], dim=-1)\n",
    "\n",
    "    boxes1 = torch.cat([torch.min(boxes1[..., :2], boxes1[..., 2:]),\n",
    "                        torch.max(boxes1[..., :2], boxes1[..., 2:])], dim=-1)\n",
    "    boxes2 = torch.cat([torch.min(boxes2[..., :2], boxes2[..., 2:]),\n",
    "                        torch.max(boxes2[..., :2], boxes2[..., 2:])], dim=-1)\n",
    "\n",
    "    # (x2 minus x1 = width)  * (y2 - y1 = height)\n",
    "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
    "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
    "\n",
    "    # upper left of the intersection region (x,y)\n",
    "    inter_left_up = torch.max(boxes1[..., :2], boxes2[..., :2])\n",
    "\n",
    "    # bottom right of the intersection region (x,y)\n",
    "    inter_right_down = torch.min(boxes1[..., 2:], boxes2[..., 2:])\n",
    "\n",
    "    # if there is overlapping we will get (w,h) else set to (0,0) because it could be negative if no overlapping\n",
    "    inter_section = torch.max(inter_right_down - inter_left_up, torch.zeros_like(inter_right_down))\n",
    "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
    "    union_area = boxes1_area + boxes2_area - inter_area\n",
    "    ious = 1.0 * inter_area / union_area\n",
    "    # cal outer boxes\n",
    "    outer_left_up = torch.min(boxes1[..., :2], boxes2[..., :2])\n",
    "    outer_right_down = torch.max(boxes1[..., 2:], boxes2[..., 2:])\n",
    "    outer = torch.max(outer_right_down - outer_left_up, torch.zeros_like(inter_right_down))\n",
    "    outer_diagonal_line = torch.pow(outer[..., 0], 2) + torch.pow(outer[..., 1], 2)\n",
    "\n",
    "    # cal center distance\n",
    "    # center x center y\n",
    "    boxes1_center = (boxes1[..., :2] +  boxes1[...,2:]) * 0.5\n",
    "    boxes2_center = (boxes2[..., :2] +  boxes2[...,2:]) * 0.5\n",
    "\n",
    "    # euclidean distance\n",
    "    # x1-x2 square \n",
    "    center_dis = torch.pow(boxes1_center[...,0]-boxes2_center[...,0], 2) +\\\n",
    "                 torch.pow(boxes1_center[...,1]-boxes2_center[...,1], 2)\n",
    "\n",
    "    # cal penalty term\n",
    "    # cal width,height\n",
    "    boxes1_size = torch.max(boxes1[..., 2:] - boxes1[..., :2], torch.zeros_like(inter_right_down))\n",
    "    boxes2_size = torch.max(boxes2[..., 2:] - boxes2[..., :2], torch.zeros_like(inter_right_down))\n",
    "    v = (4 / (math.pi ** 2)) * torch.pow(\n",
    "            torch.atan((boxes1_size[...,0]/torch.clamp(boxes1_size[...,1],min = 1e-6))) -\n",
    "            torch.atan((boxes2_size[..., 0] / torch.clamp(boxes2_size[..., 1],min = 1e-6))), 2)\n",
    "\n",
    "    alpha = v / (1-ious+v)\n",
    "\n",
    "    #cal ciou\n",
    "    cious = ious - (center_dis / outer_diagonal_line + alpha*v)\n",
    "\n",
    "    return cious\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Psrt 4 Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To conclude, we manage to load the darknet53 weight and use it as a backbone of the model. we also successfully load the COCO dataset the the system provieded and manage to make it trained. Unfortunatly, We unable to make the whole dataset to train. since it take a lot of time. In the end, our implemented(from lab direction) mAPs and CIOU loss function are left unused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
