{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "### RTML lab report \n",
    "#### Name = ati tesakulsiri  st123009\n",
    "#### Lab 05 Mark R-CNN\n",
    "---------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask R-CNN is a Convolutional Neural Network (CNN) and state-of-the-art in terms of image segmentation. This variant of a Deep Neural Network detects objects in an image and generates a high-quality segmentation mask for each instance.\n",
    "\n",
    "For each intance of an object in an image, Mask R-CNN attempts to generate\n",
    " - A bounding box\n",
    " - Class scores\n",
    " - A segmentation mask\n",
    "\n",
    "The backbone and neck of Mask R-CNN are based on\n",
    " - A feature pyramid network (FPN)\n",
    " - ResNet\n",
    "\n",
    "### Feature Pyramid Networks\n",
    "\n",
    "We've seen the idea of the feature pyramid network (FPN) in YOLOv3 and YOLOv4. It is a feature extractor using a pyramid concept.\n",
    "We begin with ordinary progressive downsampling of the input to get a multiscale representation of the input, but rather\n",
    "than using that \"low-level\" multiscale representation directly, we progressively upsample the coarse representation of\n",
    "the input using input from the low-level feature maps. The idea is shown in the left-hand panel of the diagram above.\n",
    "By incorporating information from both the low-level \"bottom-up\" fine grained feature map and the upsampled coarser grained feature\n",
    "map in the pyramid, the fine grained representation at the bottom of the pyramid contains much more useful or more \"semantic\"\n",
    "information about the input.\n",
    "\n",
    "\n",
    "### ResNet backbone\n",
    "\n",
    "The bottom-up part of the FPN used in Mask-RCNN is ResNet. It is used similar to how\n",
    "Darknet-53 is used in YOLO. We take the classifier structure as the bottom-up half (left side) of\n",
    "the pyramid, then we add the top-down part (right side) to obtain the FPN.\n",
    "\n",
    "Mask R-CNN taps into ResNet in 4 or 5 places according to the implementation, at the ouptut of various residual blocks.\n",
    "\n",
    "### Region Proposal Network (RPN)\n",
    "\n",
    "The Faster R-CNN RPN connects to the top of the FPN pyramid. It performs classification and bounding box regression for each possible proposal.\n",
    "\n",
    "<!-- <img src=\"img/RPN.png\" title=\"RPN\" style=\"width: 600px;\" /> -->\n",
    "\n",
    "### Detection network\n",
    "\n",
    "The detection network uses the results of the RPN as well as the output of the FPN. With the RPN bounding box as input, we assign the box to one of the levels of the pyramid.\n",
    "Specifically, we use\n",
    "\n",
    "$$ k = \\left\\lfloor k_0 +\\log_2\\left( \\frac{\\sqrt{wh}}{224} \\right) \\right\\rfloor $$\n",
    "\n",
    "Then the ROIAlign block interpolates the appropriate set of features from the best level (level $k$) of the pyramid. The region is aligned and scaled to a size of\n",
    "56$\\times$56, and the resulting representation is forwarded to the mask prediction head.\n",
    "\n",
    "### Mask Head\n",
    "\n",
    "The mask head is a FCN that up-samples from the detection result, and the patch size is finally re-scaled back to the input size.\n",
    "\n",
    "### U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "The u-net is convolutional network architecture for fast and precise segmentation of images. Up to now it has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. It has won the Grand Challenge for Computer-Automated Detection of Caries in Bitewing Radiography at ISBI 2015, and it has won the Cell Tracking Challenge at ISBI 2015 on the two most challenging transmitted light microscopy categories (Phase contrast and DIC microscopy) by a large margin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## 2. Methodology\n",
    "\n",
    " 1. Evaluate the pretrained COCO model on the COCO validation set we used last week.\n",
    " 2. Download the Cityscapes dataset and run Mask R-CNN on it in inference model. \n",
    " 3. Fine tune the COCO Mask R-CNN on Cityscapes and report the results. \n",
    " 4. Try U-NET and compare with Mask R-CNN\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Result\n",
    "- the python project for COCO training with COCO and some example result from traning and evaluatiung\n",
    "``` bash\n",
    ".\n",
    "├── ba_result.py\n",
    "├── cityscaoe.py\n",
    "├── demo.ipynb\n",
    "├── eval.ipynb\n",
    "├── evalute.py\n",
    "├── image\n",
    "│   └── 001.png\n",
    "├── LICENSE\n",
    "├── maskrcnn_coco-201.pth\n",
    "├── maskrcnn_coco-202.pth\n",
    "├── maskrcnn_coco-203.pth\n",
    "├── maskrcnn_coco-204.pth\n",
    "├── maskrcnn_coco-205.pth\n",
    "├── maskrcnn_coco-206.pth\n",
    "├── maskrcnn_coco-207.pth\n",
    "├── maskrcnn_coco-208.pth\n",
    "├── maskrcnn_coco-209.pth\n",
    "├── maskrcnn_coco-210.pth\n",
    "├── maskrcnn_results.pth\n",
    "├── pytorch_mask_rcnn\n",
    "│   ├── datasets\n",
    "│   │   ├── city_turbodset.py\n",
    "│   │   ├── coco_dataset.py\n",
    "│   │   ├── coco_eval.py\n",
    "│   │   ├── generalized_dataset.py\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── __pycache__\n",
    "│   │   │   ├── coco_dataset.cpython-38.pyc\n",
    "│   │   │   ├── coco_eval.cpython-38.pyc\n",
    "│   │   │   ├── generalized_dataset.cpython-38.pyc\n",
    "│   │   │   ├── __init__.cpython-38.pyc\n",
    "│   │   │   ├── utils.cpython-38.pyc\n",
    "│   │   │   └── voc_dataset.cpython-38.pyc\n",
    "│   │   ├── utils.py\n",
    "│   │   └── voc_dataset.py\n",
    "│   ├── engine.py\n",
    "│   ├── gpu_info.json\n",
    "│   ├── gpu.py\n",
    "│   ├── __init__.py\n",
    "│   ├── model\n",
    "│   │   ├── box_ops.py\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── mask_rcnn.py\n",
    "│   │   ├── pooler.py\n",
    "│   │   ├── __pycache__\n",
    "│   │   │   ├── box_ops.cpython-38.pyc\n",
    "│   │   │   ├── __init__.cpython-38.pyc\n",
    "│   │   │   ├── mask_rcnn.cpython-38.pyc\n",
    "│   │   │   ├── pooler.cpython-38.pyc\n",
    "│   │   │   ├── roi_heads.cpython-38.pyc\n",
    "│   │   │   ├── rpn.cpython-38.pyc\n",
    "│   │   │   ├── transform.cpython-38.pyc\n",
    "│   │   │   └── utils.cpython-38.pyc\n",
    "│   │   ├── roi_heads.py\n",
    "│   │   ├── rpn.py\n",
    "│   │   ├── transform.py\n",
    "│   │   └── utils.py\n",
    "│   ├── __pycache__\n",
    "│   │   ├── engine.cpython-38.pyc\n",
    "│   │   ├── gpu.cpython-38.pyc\n",
    "│   │   ├── __init__.cpython-38.pyc\n",
    "│   │   ├── utils.cpython-38.pyc\n",
    "│   │   └── visualizer.cpython-38.pyc\n",
    "│   ├── utils.py\n",
    "│   └── visualizer.py\n",
    "├── README.md\n",
    "├── trainer_pkm.sh\n",
    "└── train.py\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and here is some result from traning\n",
    "``` bash\n",
    "472800   0.148  0.028   0.182   0.068   0.261\n",
    "472900   0.063  0.098   0.067   0.035   0.308\n",
    "\n",
    "...\n",
    "\n",
    "Evaluate annotation type *bbox*\n",
    "DONE (t=30.14s).\n",
    "Accumulating evaluation results...\n",
    "DONE (t=4.19s).\n",
    "Loading and preparing results...\n",
    "DONE (t=0.09s)\n",
    "creating index...\n",
    "index created!\n",
    "Running per image evaluation...\n",
    "Evaluate annotation type *segm*\n",
    "DONE (t=31.85s).\n",
    "Accumulating evaluation results...\n",
    "DONE (t=4.24s).\n",
    "accumulate: 70.7s\n",
    "training: 389.9 s, evaluation: 681.0 s\n",
    "{'bbox AP': 20.5, 'mask AP': 16.2}\n",
    "\n",
    "total time of this training: 2165.4 s\n",
    "already trained: 5 epochs\n",
    "```\n",
    "\n",
    "> And here is the result from `evaluating`\n",
    "``` bash\n",
    "root@0746646b7288:~/keep_lab/RTML_Labsession/05_MARKRCNN/pyProj# python3 evalute.py \n",
    "...\n",
    "\n",
    "device: cuda:2\n",
    "loading annotations into memory...\n",
    "Done (t=0.85s)\n",
    "creating index...\n",
    "index created!\n",
    "Namespace(ckpt_path='/root/keep_lab/RTML_Labsession/05_MARKRCNN/pyProj/maskrcnn_coco-210.pth', data_dir='/root/Datasets/coco', dataset='coco', device_num='2', iters=3, results='/root/keep_lab/RTML_Labsession/05_MARKRCNN/pyProj/maskrcnn_results.pth', use_cuda=True)\n",
    "\n",
    "evaluating...\n",
    "\n",
    "...\n",
    "\n",
    "DONE (t=0.05s).\n",
    "Accumulating evaluation results...\n",
    "DONE (t=0.10s).\n",
    "accumulate: 0.3s\n",
    "{'bbox AP': 23.5, 'mask AP': 15.6}\n",
    "\n",
    "Total time of this evaluation: 5.9 s, speed: 3.3 imgs/s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Download the Cityscapes dataset and run Mask R-CNN on it in inference model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "root@0746646b7288:~/Datasets/Cityscapes# ls\n",
    "annotations  checked_train2017.txt  checked_train.txt  checked_val2017.txt  checked_val.txt  gtCoarse  gtFine  leftImg8bit  leftImg8bit_trainextra.zip  license.txt  README\n",
    "root@0746646b7288:~/Datasets/Cityscapes# \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fine tune the COCO Mask R-CNN on Cityscapes and report the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Compare with U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
