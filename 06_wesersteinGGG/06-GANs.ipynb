{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: Generative Adversarial Networks (GANs)\n",
    "\n",
    "In this lab, we will develop several basic GANs and experiment with them.\n",
    "\n",
    "Some of the information in this lab is based on material from Building Basic Generative Adversarial Networks (GANs) in Coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models vs Discriminative Models\n",
    "\n",
    "Discriminative models are typically used for classification in machine learning.\n",
    "Discriminative classifiers take a set of features $x$, such as having a nose or wheels,\n",
    "and from these features determine a category $y$, \n",
    "meaning that they try to model the probability of class $y$ given the set of features $x$.\n",
    "Assuming $X$ is a random variable over sets of features and $Y$ is a random variable over sets of possible classes,\n",
    "a discriminative models estimates\n",
    "\n",
    "$$D(x) = P(Y=y \\mid X=x).$$\n",
    "\n",
    "Generative models, however, model $P(x)$ or $P(x \\mid y)$.\n",
    "Generative models based on sampling take a random input and sometimes also a class $y$ such as a \"dog.\"\n",
    "From these inputs, a generative sampler will attempt to generate a set of features $x$ that are\n",
    "representative of the class \"dog.\" The random noise input ensures that we don't generate the same\n",
    "dog each time.\n",
    "\n",
    "Assuming random noise distribution $N$, a conditional sample-based generative model attempts\n",
    "the following:\n",
    "\n",
    "1. Input class $y$\n",
    "2. $z \\sim N$\n",
    "3. $x = G(z,y)$\n",
    "\n",
    "the goal is $P_{z\\sim N}(G(z,y)=x) = P(x \\mid y)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANS)\n",
    "\n",
    "GANs for images are composed of two models, a generator that generates images\n",
    "and a discriminator that is a discriminative classifier.\n",
    "The generator takes in a random noise input and an optional class and\n",
    "deterministically transforms the input into an image. The discriminator attempts to \n",
    "determine which of its inputs are real samples from the data distribution and which ones\n",
    "are fake samples generated by the generator. Over time, the models compete. If the training is\n",
    "set up well, when complete, the generator can take in any random noise input and produce a realistic result.\n",
    "In summary, $G$ learns to produce realistic examples like an artist painting paintings that look like photos,\n",
    "while $D$ distinguishes the painted photos from real photos.\n",
    "The basic GAN model described by Goodfellow et al. (2014) looks like this:\n",
    "\n",
    "<img src=\"img/gan_architecture-1.png\" title=\"GAN Framework\" style=\"width: 640px;\" />\n",
    "\n",
    "After this lab, you may be interested in reading about [6 GAN Architectures You Really Should Know](https://neptune.ai/blog/6-gan-architectures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gan Setup\n",
    "\n",
    "Let's build our first GAN.\n",
    "\n",
    "There are about a million tutorials on coding GANs with PyTorch available online. We'll use\n",
    "[code from GitHub user diegoalejogm](https://github.com/diegoalejogm/gans).\n",
    "\n",
    "To run this code you'll need some dependencies such as tensorboardX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 464 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.22.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (23.0)\n",
      "Collecting protobuf<4,>=3.8.0\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 98 kB/s eta 0:00:012\n",
      "\u001b[?25hInstalling collected packages: protobuf, tensorboardX\n",
      "Successfully installed protobuf-3.20.3 tensorboardX-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a `Logger` class with useful tricks to indicate training progress and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import errno\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "'''\n",
    "    TensorBoard Data will be stored in './runs' path\n",
    "'''\n",
    "\n",
    "\n",
    "class Logger:\n",
    "\n",
    "    def __init__(self, model_name, data_name):\n",
    "        self.model_name = model_name\n",
    "        self.data_name = data_name\n",
    "\n",
    "        self.comment = '{}_{}'.format(model_name, data_name)\n",
    "        self.data_subdir = '{}/{}'.format(model_name, data_name)\n",
    "\n",
    "        # TensorBoard\n",
    "        self.writer = SummaryWriter(comment=self.comment)\n",
    "\n",
    "    def log(self, d_error, g_error, epoch, n_batch, num_batches):\n",
    "\n",
    "        # var_class = torch.autograd.variable.Variable\n",
    "        if isinstance(d_error, torch.autograd.Variable):\n",
    "            d_error = d_error.data.cpu().numpy()\n",
    "        if isinstance(g_error, torch.autograd.Variable):\n",
    "            g_error = g_error.data.cpu().numpy()\n",
    "\n",
    "        step = Logger._step(epoch, n_batch, num_batches)\n",
    "        self.writer.add_scalar(\n",
    "            '{}/D_error'.format(self.comment), d_error, step)\n",
    "        self.writer.add_scalar(\n",
    "            '{}/G_error'.format(self.comment), g_error, step)\n",
    "\n",
    "    def log_images(self, images, num_images, epoch, n_batch, num_batches, format='NCHW', normalize=True):\n",
    "        '''\n",
    "        input images are expected in format (NCHW)\n",
    "        '''\n",
    "        if type(images) == np.ndarray:\n",
    "            images = torch.from_numpy(images)\n",
    "        \n",
    "        if format=='NHWC':\n",
    "            images = images.transpose(1,3)\n",
    "        \n",
    "\n",
    "        step = Logger._step(epoch, n_batch, num_batches)\n",
    "        img_name = '{}/images{}'.format(self.comment, '')\n",
    "\n",
    "        # Make horizontal grid from image tensor\n",
    "        horizontal_grid = vutils.make_grid(\n",
    "            images, normalize=normalize, scale_each=True)\n",
    "        # Make vertical grid from image tensor\n",
    "        nrows = int(np.sqrt(num_images))\n",
    "        grid = vutils.make_grid(\n",
    "            images, nrow=nrows, normalize=True, scale_each=True)\n",
    "\n",
    "        # Add horizontal images to tensorboard\n",
    "        self.writer.add_image(img_name, horizontal_grid, step)\n",
    "\n",
    "        # Save plots\n",
    "        self.save_torch_images(horizontal_grid, grid, epoch, n_batch)\n",
    "\n",
    "    def save_torch_images(self, horizontal_grid, grid, epoch, n_batch, plot_horizontal=True):\n",
    "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "\n",
    "        # Plot and save horizontal\n",
    "        fig = plt.figure(figsize=(16, 16))\n",
    "        plt.imshow(np.moveaxis(horizontal_grid.numpy(), 0, -1))\n",
    "        plt.axis('off')\n",
    "        if plot_horizontal:\n",
    "            display.display(plt.gcf())\n",
    "        self._save_images(fig, epoch, n_batch, 'hori')\n",
    "        plt.close()\n",
    "\n",
    "        # Save squared\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(np.moveaxis(grid.numpy(), 0, -1))\n",
    "        plt.axis('off')\n",
    "        self._save_images(fig, epoch, n_batch)\n",
    "        plt.close()\n",
    "\n",
    "    def _save_images(self, fig, epoch, n_batch, comment=''):\n",
    "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "        fig.savefig('{}/{}_epoch_{}_batch_{}.png'.format(out_dir,\n",
    "                                                         comment, epoch, n_batch))\n",
    "\n",
    "    def display_status(self, epoch, num_epochs, n_batch, num_batches, d_error, g_error, d_pred_real, d_pred_fake):\n",
    "        \n",
    "        # var_class = torch.autograd.variable.Variable\n",
    "        if isinstance(d_error, torch.autograd.Variable):\n",
    "            d_error = d_error.data.cpu().numpy()\n",
    "        if isinstance(g_error, torch.autograd.Variable):\n",
    "            g_error = g_error.data.cpu().numpy()\n",
    "        if isinstance(d_pred_real, torch.autograd.Variable):\n",
    "            d_pred_real = d_pred_real.data\n",
    "        if isinstance(d_pred_fake, torch.autograd.Variable):\n",
    "            d_pred_fake = d_pred_fake.data\n",
    "        \n",
    "        \n",
    "        print('Epoch: [{}/{}], Batch Num: [{}/{}]'.format(\n",
    "            epoch,num_epochs, n_batch, num_batches)\n",
    "             )\n",
    "        print('Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(d_error, g_error))\n",
    "        print('D(x): {:.4f}, D(G(z)): {:.4f}'.format(d_pred_real.mean(), d_pred_fake.mean()))\n",
    "\n",
    "    def save_models(self, generator, discriminator, epoch):\n",
    "        out_dir = './data/models/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "        torch.save(generator.state_dict(),\n",
    "                   '{}/G_epoch_{}'.format(out_dir, epoch))\n",
    "        torch.save(discriminator.state_dict(),\n",
    "                   '{}/D_epoch_{}'.format(out_dir, epoch))\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "    # Private Functionality\n",
    "\n",
    "    @staticmethod\n",
    "    def _step(epoch, n_batch, num_batches):\n",
    "        return epoch * num_batches + n_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_dir(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla GAN for MNIST dataset\n",
    "\n",
    "Next we'll download the MNIST dataset as a small dataset on which we can get things running quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./torch_data/VGAN/MNIST/dataset/MNIST/raw/train-images-idx3-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n",
      "9.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./torch_data/VGAN/MNIST/dataset/MNIST/raw/train-labels-idx1-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "112.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./torch_data/VGAN/MNIST/dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./torch_data/VGAN/MNIST/dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "DATA_FOLDER = './torch_data/VGAN/MNIST'\n",
    "def mnist_data():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "    out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "\n",
    "# Load Dataset and attach a DataLoader\n",
    "\n",
    "data = mnist_data()\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The generator in a GAN is the model\n",
    "you want to help achieve high performance.\n",
    "A generator generates different objects because of the random noise sample.\n",
    "If we make small changes to the noise, we should be able to see corresponding\n",
    "small changes to the output. The generator is driven by a noise vector\n",
    "sampled from a latent space (the domain of $p_z$) and transforms that\n",
    "noise sample into an element of the domain of\n",
    "$p_{data}$.\n",
    "\n",
    "<img src=\"img/Generator.jpg\" title=\"Generator\" style=\"width: 640px;\" />\n",
    "\n",
    "The generator model can be practically anything that has the right input\n",
    "and output tensor shapes. The \"vanilla\" GAN is the simplest GAN network architecture.\n",
    "Here is the structure of a simple vanilla GAN generator using only\n",
    "fully connected layers:\n",
    "\n",
    "<img src=\"img/VanillaGAN-Gen.png\" title=\"Generator model\" style=\"width: 640px;\" />\n",
    "\n",
    "And here is sample code for the model's PyTorch Module. Note that since we\n",
    "normalize the real-valued input data to the range [-1,1], to limit the generator to the\n",
    "same range, we use a hyperbolic tangent activation at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer generative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        n_features = 100\n",
    "        n_out = 784\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024, n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# Function to create noise samples for the generator's input\n",
    "\n",
    "def noise(size):\n",
    "    n = torch.randn(size, 100)\n",
    "    if torch.cuda.is_available(): return n.cuda() \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator is a type of classifier, but it is just to classify its input as\n",
    "real or fake.\n",
    "When a fake sample from the generator is given, it should ouptut 0 for fake:\n",
    "\n",
    "<img src=\"img/DiscriminatorFake.png\" title=\"Discriminator-1\" style=\"width: 640px;\" />\n",
    "\n",
    "On the other hand, if the input is real, it shoudl output 1 for real:\n",
    "\n",
    "<img src=\"img/DiscriminatorReal.jpg\" title=\"Discriminator-2\" style=\"width: 640px;\" />\n",
    "\n",
    "We'll use the following simple discriminator structure:\n",
    "\n",
    "<img src=\"img/VanillaGAN-Dis.png\" title=\"VanillaGAN Discriminator\" style=\"width: 640px;\" />\n",
    "\n",
    "Here is the Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer discriminative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        n_features = 784\n",
    "        n_out = 1\n",
    "        \n",
    "        self.hidden0 = nn.Sequential( \n",
    "            nn.Linear(n_features, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(256, n_out),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "def images_to_vectors(images):\n",
    "    return images.view(images.size(0), 784)\n",
    "\n",
    "def vectors_to_images(vectors):\n",
    "    return vectors.view(vectors.size(0), 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the modules\n",
    "\n",
    "Let's create an instance of the generator and discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = DiscriminatorNet()\n",
    "generator = GeneratorNet()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    discriminator.cuda()\n",
    "    generator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the optimizers\n",
    "\n",
    "The optimization is a min-max game.\n",
    "The generator wants to minimize the objective function, whereas the discriminator wants to maximize the same objective function.\n",
    "The discriminator's loss function is binary cross entropy:\n",
    "\n",
    "$$\\mathcal{L}_D = \\max_D\\mathcal{L}(D;G)=\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_x{z}}[\\log (1-D(G(z))]$$\n",
    "\n",
    "The generator does't affect the first term of $\\mathcal{L}_D$, so its goal\n",
    "is a bit simpler, to minimize the second term of $\\mathcal{L}_D$:\n",
    "\n",
    "$$\\mathcal{L}_G = \\min_G\\mathcal{L}(G;D)=\\mathbb{E}_{z \\sim p_x{z}}[\\log (D(G(z))]$$\n",
    "\n",
    "Putting these together we have\n",
    "\n",
    "$$\\min_G\\max_D\\mathcal{L}(D;G)=\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_x{z}}[\\log (1-D(G(z))]$$\n",
    "\n",
    "Here is a diagram of the objective function:\n",
    "\n",
    "<img src=\"img/GanObjectivefunction.png\" title=\"min-max optimization\" style=\"width: 640px;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: why not select a very strong discriminator?\n",
    "\n",
    "If we have a discriminator that is far superior to the generator, it will quickly determine that all the fake examples are 100% fake with\n",
    "high confidence. That will not be very useful for the generator, which needs a signal to tell it how to make its samples look less fake.\n",
    "On the other hand, if we have a generator that is far superior to the discriminator, we will get predictions indicating that both the real and\n",
    "generated samples are equally likely to be real or fake. This is actually the end goal: a perfect generator. But we are unlikely to obtain a\n",
    "perfect generator, and if the generator is not yet perfect, it is important to keep the generator and discriminator both improving together,\n",
    "with similar \"skill levels\" from the beginning. Since the discriminator has an \"easier\" job than the generator, it will be difficult to keep\n",
    "the competition balanced. We will talk about some solutions to this problem, such as WGANs, later.\n",
    "\n",
    "OK, back to the optimizers.\n",
    "From Goodfellow et al. (2014), the two networks are trained in an alternating fashion.\n",
    "So it will be straightforward to have a separate optimizer for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss function\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# How many epochs to train for\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# Number of steps to apply to the discriminator for each step of the generator (1 in Goodfellow et al.)\n",
    "\n",
    "d_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The targets for the discriminator may be 0 or 1 depending on whether we're giving it\n",
    "real or fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = torch.ones(size, 1)\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def fake_data_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = torch.zeros(size, 1)\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function for a single step for the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Propagate real data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    error_real = loss(prediction_real, real_data_target(real_data.size(0)))\n",
    "    error_real.backward()\n",
    "\n",
    "    # Propagate fake data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    error_fake = loss(prediction_fake, fake_data_target(real_data.size(0)))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    # Take a step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error_real + error_fake, prediction_real, prediction_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a function for a single step of the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(optimizer, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Propagate the fake data through the discriminator and backpropagate.\n",
    "    # Note that since we want the generator to output something that gets\n",
    "    # the discriminator to output a 1, we use the real data target here.\n",
    "    prediction = discriminator(fake_data)\n",
    "    error = loss(prediction, real_data_target(prediction.size(0)))\n",
    "    error.backward()\n",
    "    \n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test noise samples\n",
    "\n",
    "Let's generate some noise vectors to use as inputs to the generator.\n",
    "We'll use these samples repeatedly to see the evolution of the generator\n",
    "over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 16\n",
    "test_noise = noise(num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training\n",
    "\n",
    "Now let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27/200], Batch Num: [300/600]\n",
      "Discriminator Loss: 0.9962, Generator Loss: 1.3992\n",
      "D(x): 0.6887, D(G(z)): 0.3478\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m\n\u001b[1;32m     28\u001b[0m     logger\u001b[39m.\u001b[39mdisplay_status(\n\u001b[1;32m     29\u001b[0m         epoch, num_epochs, n_batch, num_batches,\n\u001b[1;32m     30\u001b[0m         d_error, g_error, d_pred_real, d_pred_fake\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[39m# Save model checkpoints\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m logger\u001b[39m.\u001b[39;49msave_models(generator, discriminator, epoch)\n",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m, in \u001b[0;36mLogger.save_models\u001b[0;34m(self, generator, discriminator, epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m Logger\u001b[39m.\u001b[39m_make_dir(out_dir)\n\u001b[1;32m    117\u001b[0m torch\u001b[39m.\u001b[39msave(generator\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m    118\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/G_epoch_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(out_dir, epoch))\n\u001b[0;32m--> 119\u001b[0m torch\u001b[39m.\u001b[39;49msave(discriminator\u001b[39m.\u001b[39;49mstate_dict(),\n\u001b[1;32m    120\u001b[0m            \u001b[39m'\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m/D_epoch_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(out_dir, epoch))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[39mSaves an object to a disk file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m--> 376\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    377\u001b[0m     \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    378\u001b[0m         \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger = Logger(model_name='VGAN', data_name='MNIST')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, (real_batch,_) in enumerate(data_loader):\n",
    "\n",
    "        # Train discriminator on a real batch and a fake batch\n",
    "        \n",
    "        real_data = images_to_vectors(real_batch)\n",
    "        if torch.cuda.is_available(): real_data = real_data.cuda()\n",
    "        fake_data = generator(noise(real_data.size(0))).detach()\n",
    "        d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer,\n",
    "                                                                real_data, fake_data)\n",
    "        \n",
    "        # Train generator\n",
    "\n",
    "        fake_data = generator(noise(real_batch.size(0)))\n",
    "        g_error = train_generator(g_optimizer, fake_data)\n",
    "        \n",
    "        # Log errors and display progress\n",
    "\n",
    "        logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
    "        if (n_batch) % 100 == 0:\n",
    "            display.clear_output(True)\n",
    "            # Display Images\n",
    "            test_images = vectors_to_images(generator(test_noise)).data.cpu()\n",
    "            logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches);\n",
    "            # Display status Logs\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, n_batch, num_batches,\n",
    "                d_error, g_error, d_pred_real, d_pred_fake\n",
    "            )\n",
    "            \n",
    "        # Save model checkpoints\n",
    "        logger.save_models(generator, discriminator, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN for CIFAR\n",
    "\n",
    "Our simple generator for MNIST was fully connected. While that worked\n",
    "fine for MNIST, it would not work for more complex RGB images.\n",
    "\n",
    "The DCGAN is a GAN with a generator\n",
    "designed to do just that, generate larger RGB images using convolutional layers.\n",
    "\n",
    "Here is the DCGAN architecture:\n",
    "\n",
    "<img src=\"img/dcgan.png\" title=\"DCGAN\" style=\"width: 1080px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR10 data\n",
    "\n",
    "For our DCGAN experiment, we'll use CIFAR rather than MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './torch_data/DCGAN/CIFAR'\n",
    "\n",
    "def cifar_data():\n",
    "    compose = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(64),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "        ])\n",
    "    out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
    "    return datasets.CIFAR10(root=out_dir, train=True, transform=compose, download=True)\n",
    "\n",
    "data = cifar_data()\n",
    "batch_size = 100\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks\n",
    "\n",
    "Here's another simpler picture of the DCGAN model structures:\n",
    "\n",
    "<img src=\"img/dcgan2.png\" title=\"DCGAN\" style=\"width: 640px;\" />\n",
    "\n",
    "Here are PyTorch Modules for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminativeNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DiscriminativeNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, out_channels=128, kernel_size=4, \n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128, out_channels=256, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=256, out_channels=512, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=512, out_channels=1024, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024*4*4, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        # Flatten and apply sigmoid\n",
    "        x = x.view(-1, 1024*4*4)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator is using transpose convolutions with batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GenerativeNet, self).__init__()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(100, 1024*4*4)\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=1024, out_channels=512, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=512, out_channels=256, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=256, out_channels=128, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=128, out_channels=3, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            )\n",
    "        )\n",
    "        self.out = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project and reshape\n",
    "        x = self.linear(x)\n",
    "        x = x.view(x.shape[0], 1024, 4, 4)\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        # Apply Tanh\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a generator and discriminator and initialize their weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weight initialization\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.00, 0.02)\n",
    "\n",
    "# Instantiate networks\n",
    "\n",
    "generator = GenerativeNet()\n",
    "generator.apply(init_weights)\n",
    "discriminator = DiscriminativeNet()\n",
    "discriminator.apply(init_weights)\n",
    "\n",
    "# Enable cuda if available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "Next, we set up the optimizers and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss function\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# Number of epochs of training\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "As before, we'll generate a fixed set of noise samples to see the evolution of the\n",
    "generator over time then start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 16\n",
    "test_noise = noise(num_test_samples)\n",
    "\n",
    "logger = Logger(model_name='DCGAN', data_name='CIFAR10')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, (real_data,_) in enumerate(data_loader):\n",
    "\n",
    "        # Train Discriminator\n",
    "        \n",
    "        if torch.cuda.is_available(): real_data = real_data.cuda()\n",
    "        fake_data = generator(noise(real_data.size(0))).detach()\n",
    "        d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer, \n",
    "                                                                real_data, fake_data)\n",
    "\n",
    "        # Train Generator\n",
    "        \n",
    "        fake_data = generator(noise(real_batch.size(0)))\n",
    "        g_error = train_generator(g_optimizer, fake_data)\n",
    "\n",
    "        # Log error and display progress\n",
    "        logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
    "        if (n_batch) % 100 == 0:\n",
    "            display.clear_output(True)\n",
    "            # Display Images\n",
    "            test_images = generator(test_noise).data.cpu()\n",
    "            logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches);\n",
    "            # Display status Logs\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, n_batch, num_batches,\n",
    "                d_error, g_error, d_pred_real, d_pred_fake\n",
    "            )\n",
    "\n",
    "        # Save model checkpoints\n",
    "        logger.save_models(generator, discriminator, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GANs\n",
    "\n",
    "### Unconditional GANs\n",
    "\n",
    "Until now, we know how to use GANs to generate some images. However, we never know what is the image classes which it generated. We call the GANs at above that **Unconditional GANs**.\n",
    "The unconditional GANs may useful for creating some dataset, but it is not good that we don't know which images will be generated.\n",
    "\n",
    "### Conditional GANs\n",
    "\n",
    "From the problem above, we want the generator to generate the specific data more than randomize the output, so **conditional GANs** has been created to address it. Conditional generation allows you to get the sample from a specific class.\n",
    "\n",
    "The thing you need for train the conditional GANs is **Labeled class**.\n",
    "\n",
    "| Unconditional GANs  | Conditional GANs |\n",
    "|:---:|:---:|\n",
    "| Get random samples from **random classes** | Get random sample from **specific class** |\n",
    "| Training dataset does **not require labeled classes**. | Training dataset **requires labeled classes**. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using conditional GANs\n",
    "\n",
    "In generator input, you need to add (concat) noise vector and **one-hot vector class** into the training as image below.\n",
    "\n",
    "<img src=\"img/Conditional_generator.png\" title=\"Conditional Generator\" style=\"width: 800px;\" />\n",
    "\n",
    "In the same time, for discriminator, to tell discriminator that we must check the image with class, you need to add the one-hot vector class into the input of discriminator too.\n",
    "\n",
    "<img src=\"img/Conditional_discriminator.png\" title=\"Conditional Discriminator\" style=\"width: 800px;\" />\n",
    "\n",
    "The coding is at below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for our testing purposes, please do not change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Generator and Noise vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, input_dim=10, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "      im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "            (MNIST is black-and-white, so 1 channel is your default)\n",
    "      hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_chan=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_chan)\n",
    "        '''\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Input\n",
    "\n",
    "In conditional GANs, the input vector for the generator will also need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0's and a 1 on the chosen class. Given the labels of multiple images (e.g. from a batch) and number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def get_one_hot_labels(labels, n_classes):\n",
    "    '''\n",
    "    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n",
    "    Parameters:\n",
    "        labels: tensor of labels from the dataloader, size (?)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "    '''\n",
    "    return F.one_hot(labels, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. You will also need to do this when adding the class channels to the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_vectors(x, y):\n",
    "    '''\n",
    "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n",
    "    Parameters:\n",
    "      x: (n_samples, ?) the first vector. \n",
    "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n",
    "        but you shouldn't need to know the second dimension's size.\n",
    "      y: (n_samples, ?) the second vector.\n",
    "        Once again, in this assignment this will be the one-hot class vector \n",
    "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n",
    "    '''\n",
    "    # Note: Make sure this function outputs a float no matter what inputs it receives\n",
    "    combined = torch.cat((x.float(), y.float()), dim=1)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now you can start to put it all together!\n",
    "First, you will define some new parameters:\n",
    "\n",
    " - mnist_shape: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it's black-and-white) so 1 x 28 x 28\n",
    " - n_classes: the number of classes in MNIST (10, since there are the digits from 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_shape = (1, 28, 28)\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 200\n",
    "z_dim = 64\n",
    "display_step = 1000\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST('.', download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can initialize your generator, discriminator, and optimizers. To do this, you will need to update the input dimensions for both models.\n",
    "\n",
    "**For the generator**\n",
    "\n",
    "you will need to calculate the size of the input vector; recall that for conditional GANs, the generator's input is the noise vector concatenated with the class vector.\n",
    "\n",
    "**For the discriminator**\n",
    "\n",
    "you need to add a channel for every class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dimensions(z_dim, mnist_shape, n_classes):\n",
    "    '''\n",
    "    Function for getting the size of the conditional input dimensions \n",
    "    from z_dim, the image shape, and number of classes.\n",
    "    Parameters:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "                (10 for MNIST)\n",
    "    Returns: \n",
    "        generator_input_dim: the input dimensionality of the conditional generator, \n",
    "                          which takes the noise and class vectors\n",
    "        discriminator_im_chan: the number of input channels to the discriminator\n",
    "                            (e.g. C x 28 x 28 for MNIST)\n",
    "    '''\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "    discriminator_im_chan = mnist_shape[0] + n_classes\n",
    "    return generator_input_dim, discriminator_im_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "#UNIT TEST NOTE: Initializations needed for grading\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1], mnist_shape[2])\n",
    "\n",
    "        ### Update discriminator ###\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size \n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        # Now you can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "       \n",
    "        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
    "        fake = gen(noise_and_labels)\n",
    "\n",
    "        # Now you can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels, \n",
    "        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "        \n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "        \n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step() \n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "        ### Update generator ###\n",
    "        # Zero out the generator gradients\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if you didn't concatenate your labels to your image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "        #\n",
    "\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            print(f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n",
    "            show_tensor_images(fake)\n",
    "            show_tensor_images(real)\n",
    "            step_bins = 20\n",
    "            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Discriminator Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\")\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you got error on **RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED using pytorch**\n",
    "It's mean that your CUDA version is not 11.2 or upper, so you need to install another torch version.\n",
    "\n",
    "<code>pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "#### Changing the Class Vector\n",
    "You can generate some numbers with your new model! You can add interpolation as well to make it more interesting.\n",
    "\n",
    "So starting from a image, you will produce intermediate images that look more and more like the ending image until you get to the final image. Your're basically morphing one image into another. You can choose what these two images will be using your conditional GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = gen.eval()\n",
    "\n",
    "import math\n",
    "\n",
    "### Change me! ###\n",
    "n_interpolation = 9 # Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)\n",
    "interpolation_noise = get_noise(1, z_dim, device=device).repeat(n_interpolation, 1)\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    first_label = get_one_hot_labels(torch.Tensor([first_number]).long(), n_classes)\n",
    "    second_label = get_one_hot_labels(torch.Tensor([second_number]).long(), n_classes)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels\n",
    "    percent_second_label = torch.linspace(0, 1, n_interpolation)[:, None]\n",
    "    interpolation_labels = first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "\n",
    "    # Combine the noise and the labels\n",
    "    noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device))\n",
    "    fake = gen(noise_and_labels)\n",
    "    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)\n",
    "\n",
    "### Change me! ###\n",
    "start_plot_number = 1 # Choose the start digit\n",
    "### Change me! ###\n",
    "end_plot_number = 5 # Choose the end digit\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "interpolate_class(start_plot_number, end_plot_number)\n",
    "_ = plt.axis('off')\n",
    "\n",
    "### Uncomment the following lines of code if you would like to visualize a set of pairwise class \n",
    "### interpolations for a collection of different numbers, all in a single grid of interpolations.\n",
    "### You'll also see another visualization like this in the next code block!\n",
    "plot_numbers = [2, 3, 4, 5, 7]\n",
    "n_numbers = len(plot_numbers)\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, first_plot_number in enumerate(plot_numbers):\n",
    "    for j, second_plot_number in enumerate(plot_numbers):\n",
    "        plt.subplot(n_numbers, n_numbers, i * n_numbers + j + 1)\n",
    "        interpolate_class(first_plot_number, second_plot_number)\n",
    "        plt.axis('off')\n",
    "plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the Noise Vector\n",
    "Now, what happens if you hold the class constant, but instead you change the noise vector? You can also interpolate the noise vector and generate an image at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_interpolation = 9 # How many intermediate images you want + 2 (for the start and end image)\n",
    "\n",
    "# This time you're interpolating between the noise instead of the labels\n",
    "for i in range(10):\n",
    "    #interpolation_label = get_one_hot_labels(torch.Tensor([5]).long(), n_classes).repeat(n_interpolation, 1).float()\n",
    "    interpolation_label = get_one_hot_labels(torch.Tensor([i]).long(), n_classes).repeat(n_interpolation, 1).float()\n",
    "\n",
    "    def interpolate_noise(first_noise, second_noise):\n",
    "        # This time you're interpolating between the noise instead of the labels\n",
    "        percent_first_noise = torch.linspace(0, 1, n_interpolation)[:, None].to(device)\n",
    "        interpolation_noise = first_noise * percent_first_noise + second_noise * (1 - percent_first_noise)\n",
    "\n",
    "        # Combine the noise and the labels again\n",
    "        noise_and_labels = combine_vectors(interpolation_noise, interpolation_label.to(device))\n",
    "        fake = gen(noise_and_labels)\n",
    "        show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)\n",
    "\n",
    "    # Generate noise vectors to interpolate between\n",
    "    ### Change me! ###\n",
    "    #for n_noise in range(10):\n",
    "    n_noise = 5 # Choose the number of noise examples in the grid\n",
    "    plot_noises = [get_noise(1, z_dim, device=device) for i in range(n_noise)]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i, first_plot_noise in enumerate(plot_noises):\n",
    "        for j, second_plot_noise in enumerate(plot_noises):\n",
    "            plt.subplot(n_noise, n_noise, i * n_noise + j + 1)\n",
    "            interpolate_noise(first_plot_noise, second_plot_noise)\n",
    "            plt.axis('off')\n",
    "    plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controllable Generation\n",
    "\n",
    "Controllable generation allows you to control some of the features that you want in your output examples. For instance, with a gan that performs face generation, you could control the age of the person's looks in the image or if they have sunglasses or the direction they're looking at in the picture, or they're perceived gender. You can do this by actually tweaking the input noise vector Z, that is fed to the generator after you train the model. For example:\n",
    "- Glasses\n",
    "- Gender\n",
    "- Age\n",
    "- Hair color\n",
    "\n",
    "<img src=\"img/Controllable_generation.png\" title=\"Controllable generation\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controllable vs Conditional GANs\n",
    "\n",
    "Controllable and conditional GANs are looked-a-like. Because some features are also can be a class, and sometime Controllable GANs also include conditional GANs in the network. However, there are some specific differences in there\n",
    "\n",
    "| Controllable | Conditional |\n",
    "|:---:|:---:|\n",
    "| Generate the samples with features | Generate the sample with the specific class |\n",
    "| Labeled data is not required | Require labeled data |\n",
    "| Manipulate the z vector input | append a class vector to the input |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Z-Space\n",
    "\n",
    "To control the output features from GANs, It is necessary to find the vector Z-space direction. Then, you can modify the output by moving the noise input vector in that known direction. \n",
    "\n",
    "<img src=\"img/Controllable_vectorz.png\" title=\"Vector Z-space direction\" style=\"width: 640px;\" />\n",
    "\n",
    "However, the Vector Z-space may have conflict with other features when the Z noise is too low data.\n",
    "In the other hand, if the Z noise is too much, from \"a normal woman\" you may get \"a woman with mustache\" instead of \"a man with mustache\". Thus, you need to be carefule about tuning the feature parameter.\n",
    "\n",
    "So to find the direction in the z-space that modifies certain features on the output, say the presence of sunglasses. Using a **train classifier that identifies features**. To do that, taking a batch of noise vector Z, that goes through the generator to get some images. For example, images through a sunglasses classifier, which will tell you at the outputs correspond to people with or without sunglasses.\n",
    "\n",
    "To use them information to modify your Z vectors, and this is without modifying the weights of the generator at all. So **the generator weights are frozen**, and modifying only the Z vectors by moving in the direction of the gradient with the costs. \n",
    "\n",
    "<img src=\"img/Controllable_howto.png\" title=\"\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Try\n",
    "\n",
    "In this case, we are going to use [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset.\n",
    "\n",
    "We have the pretrained weight of classifier and celebA in [the link](https://www.dropbox.com/sh/n593lmfeilgza14/AAAQylboARKi5DIJtX07o7pFa?dl=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CelebA\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create generator and noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "              (CelebA is rgb, so 3 is our default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(z_dim, hidden_dim * 8),\n",
    "            self.make_gen_block(hidden_dim * 8, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, z_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples in the batch, a scalar\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create classifier class\n",
    "\n",
    "As we mention above, the important thing for controllabe GANs is using classifier to adjust vector z-space, so we need to create classifier class for support the pretrained dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    '''\n",
    "    Classifier Class\n",
    "    Values:\n",
    "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "              (CelebA is rgb, so 3 is our default)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_chan=3, n_classes=2, hidden_dim=64):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.make_classifier_block(im_chan, hidden_dim),\n",
    "            self.make_classifier_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_classifier_block(hidden_dim * 2, hidden_dim * 4, stride=3),\n",
    "            self.make_classifier_block(hidden_dim * 4, n_classes, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_classifier_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a classifier block; \n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the classifier: Given an image tensor, \n",
    "        returns an n_classes-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with im_chan channels\n",
    "        '''\n",
    "        class_pred = self.classifier(image)\n",
    "        return class_pred.view(len(class_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 64\n",
    "batch_size = 128\n",
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Classifier\n",
    "\n",
    "Training your own classifier with this code, but this project provides with a pretrained one later in the code, so it is not necessary for training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(filename):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # You can run this code to train your own classifier, but there is a provided pretrained one.\n",
    "    # If you'd like to use this, just run \"train_classifier(filename)\"\n",
    "    # to train and save a classifier on the label indices to that filename.\n",
    "\n",
    "    # Target all the classes, so that's how many the classifier will learn\n",
    "    label_indices = range(40)\n",
    "\n",
    "    n_epochs = 3\n",
    "    display_step = 500\n",
    "    lr = 0.001\n",
    "    beta_1 = 0.5\n",
    "    beta_2 = 0.999\n",
    "    image_size = 64\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        CelebA(\".\", split='train', download=True, transform=transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    classifier = Classifier(n_classes=len(label_indices)).to(device)\n",
    "    class_opt = torch.optim.Adam(classifier.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    cur_step = 0\n",
    "    classifier_losses = []\n",
    "    # classifier_val_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        # Dataloader returns the batches\n",
    "        for real, labels in tqdm(dataloader):\n",
    "            real = real.to(device)\n",
    "            labels = labels[:, label_indices].to(device).float()\n",
    "\n",
    "            class_opt.zero_grad()\n",
    "            class_pred = classifier(real)\n",
    "            class_loss = criterion(class_pred, labels)\n",
    "            class_loss.backward() # Calculate the gradients\n",
    "            class_opt.step() # Update the weights\n",
    "            classifier_losses += [class_loss.item()] # Keep track of the average classifier loss\n",
    "\n",
    "            ## Visualization code ##\n",
    "            if cur_step % display_step == 0 and cur_step > 0:\n",
    "                class_mean = sum(classifier_losses[-display_step:]) / display_step\n",
    "                print(f\"Step {cur_step}: Classifier loss: {class_mean}\")\n",
    "                step_bins = 20\n",
    "                x_axis = sorted([i * step_bins for i in range(len(classifier_losses) // step_bins)] * step_bins)\n",
    "                sns.lineplot(x_axis, classifier_losses[:len(x_axis)], label=\"Classifier Loss\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                torch.save({\"classifier\": classifier.state_dict()}, filename)\n",
    "            cur_step += 1\n",
    "\n",
    "# Uncomment the last line to train your own classfier - this line will not work now.\n",
    "# If you'd like to do this, you'll have to download it and run it, ideally using a GPU \n",
    "# train_classifier(\"filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pretrained Models\n",
    "You will then load the pretrained generator and classifier using the following code. (If you trained your own classifier, you can load that one here instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "gen = Generator(z_dim).to(device)\n",
    "gen_dict = torch.load(\"pretrained_celeba.pth\", map_location=torch.device(device))[\"gen\"]\n",
    "gen.load_state_dict(gen_dict)\n",
    "gen.eval()\n",
    "\n",
    "n_classes = 40\n",
    "classifier = Classifier(n_classes=n_classes).to(device)\n",
    "class_dict = torch.load(\"pretrained_classifier.pth\", map_location=torch.device(device))[\"classifier\"]\n",
    "classifier.load_state_dict(class_dict)\n",
    "classifier.eval()\n",
    "print(\"Loaded the models!\")\n",
    "\n",
    "opt = torch.optim.Adam(classifier.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now you can start implementing a method for controlling your GAN!\n",
    "\n",
    "For training, you need to write the code to update the noise to produce more of your desired feature. You do this by performing stochastic gradient ascent. You use stochastic gradient ascent to find the local maxima, as opposed to stochastic gradient descent which finds the local minima. Gradient ascent is gradient descent over the negative of the value being optimized. Their formulas are essentially the same, however, instead of subtracting the weighted value, stochastic gradient ascent adds it; it can be calculated by `new = old + (∇ old * weight)`, where ∇ is the gradient of `old`. You perform stochastic gradient ascent to try and maximize the amount of the feature you want. If you wanted to reduce the amount of the feature, you would perform gradient descent. However, in this assignment you are interested in maximize your feature using gradient ascent, since many features in the dataset are not present much more often than they're present and you are trying to add a feature to the images, not remove.\n",
    "\n",
    "Given the noise with its gradient already calculated through the classifier, you want to return the new noise vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_updated_noise(noise, weight):\n",
    "    '''\n",
    "    Function to return noise vectors updated with stochastic gradient ascent.\n",
    "    Parameters:\n",
    "        noise: the current noise vectors. You have already called the backwards function on the target class\n",
    "          so you can access the gradient of the output class with respect to the noise by using noise.grad\n",
    "        weight: the scalar amount by which you should weight the noise gradient\n",
    "    '''\n",
    "    new_noise = noise + noise.grad * weight\n",
    "    return new_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation\n",
    "Now, you can use the classifier along with stochastic gradient ascent to make noise that generates more of a certain feature. In the code given to you here, you can generate smiling faces. Feel free to change the target index and control some of the other features in the list! You will notice that some features are easier to detect and control than others.\n",
    "\n",
    "The list you have here are the features labeled in CelebA, which you used to train your classifier. If you wanted to control another feature, you would need to get data that is labeled with that feature and train a classifier on that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First generate a bunch of images with the generator\n",
    "n_images = 8\n",
    "fake_image_history = []\n",
    "grad_steps = 10 # Number of gradient steps to take\n",
    "skip = 2 # Number of gradient steps to skip in the visualization\n",
    "\n",
    "feature_names = [\"5oClockShadow\", \"ArchedEyebrows\", \"Attractive\", \"BagsUnderEyes\", \"Bald\", \"Bangs\",\n",
    "\"BigLips\", \"BigNose\", \"BlackHair\", \"BlondHair\", \"Blurry\", \"BrownHair\", \"BushyEyebrows\", \"Chubby\",\n",
    "\"DoubleChin\", \"Eyeglasses\", \"Goatee\", \"GrayHair\", \"HeavyMakeup\", \"HighCheekbones\", \"Male\", \n",
    "\"MouthSlightlyOpen\", \"Mustache\", \"NarrowEyes\", \"NoBeard\", \"OvalFace\", \"PaleSkin\", \"PointyNose\", \n",
    "\"RecedingHairline\", \"RosyCheeks\", \"Sideburn\", \"Smiling\", \"StraightHair\", \"WavyHair\", \"WearingEarrings\", \n",
    "\"WearingHat\", \"WearingLipstick\", \"WearingNecklace\", \"WearingNecktie\", \"Young\"]\n",
    "\n",
    "### Change me! ###\n",
    "target_indices = feature_names.index(\"Male\") # Feel free to change this value to any string from feature_names!\n",
    "\n",
    "noise = get_noise(n_images, z_dim).to(device).requires_grad_()\n",
    "for i in range(grad_steps):\n",
    "    opt.zero_grad()\n",
    "    fake = gen(noise)\n",
    "    fake_image_history += [fake]\n",
    "    fake_classes_score = classifier(fake)[:, target_indices].mean()\n",
    "    fake_classes_score.backward()\n",
    "    noise.data = calculate_updated_noise(noise, 1 / grad_steps)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [n_images * 2, grad_steps * 2]\n",
    "show_tensor_images(torch.cat(fake_image_history[::skip], dim=2), num_images=n_images, nrow=n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entanglement and Regularization\n",
    "You may also notice that sometimes more features than just the target feature change. This is because some features are entangled. To fix this, you can try to isolate the target feature more by holding the classes outside of the target class constant. One way you can implement this is by penalizing the differences from the original class with L2 regularization. This L2 regularization would apply a penalty for this difference using the L2 norm and this would just be an additional term on the loss function.\n",
    "\n",
    "Here, you'll have to implement the score function: the higher, the better. The score is calculated by adding the target score and a penalty -- note that the penalty is meant to lower the score, so it should have a negative value.\n",
    "\n",
    "For every non-target class, take the difference between the current noise and the old noise. The greater this value is, the more features outside the target have changed. You will calculate the magnitude of the change, take the mean, and negate it. Finally, add this penalty to the target score. The target score is the mean of the target class in the current noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight):\n",
    "    '''\n",
    "    Function to return the score of the current classifications, penalizing changes\n",
    "    to other classes with an L2 norm.\n",
    "    Parameters:\n",
    "        current_classifications: the classifications associated with the current noise\n",
    "        original_classifications: the classifications associated with the original noise\n",
    "        target_indices: the index of the target class\n",
    "        other_indices: the indices of the other classes\n",
    "        penalty_weight: the amount that the penalty should be weighted in the overall score\n",
    "    '''\n",
    "    # Steps: 1) Calculate the change between the original and current classifications (as a tensor)\n",
    "    #           by indexing into the other_indices you're trying to preserve, like in x[:, features].\n",
    "    #        2) Calculate the norm (magnitude) of changes per example.\n",
    "    #        3) Multiply the mean of the example norms by the penalty weight. \n",
    "    #           This will be your other_class_penalty.\n",
    "    #           Make sure to negate the value since it's a penalty!\n",
    "    #        4) Take the mean of the current classifications for the target feature over all the examples.\n",
    "    #           This mean will be your target_score.\n",
    "    # Calculate the norm (magnitude) of changes per example and multiply by penalty weight\n",
    "    distance = current_classifications[:, other_indices] - original_classifications[:, other_indices]\n",
    "    other_class_penalty = -torch.norm(distance, dim=1).mean() * penalty_weight\n",
    "    # Take the mean of the current classifications for the target feature\n",
    "    target_score = current_classifications[:, target_indices].mean()\n",
    "    return target_score + other_class_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake_image_history = []\n",
    "### Change me! ###\n",
    "target_indices = feature_names.index(\"BlackHair\") # Feel free to change this value to any string from feature_names from earlier!\n",
    "other_indices = [cur_idx != target_indices for cur_idx, _ in enumerate(feature_names)]\n",
    "noise = get_noise(n_images, z_dim).to(device).requires_grad_()\n",
    "original_classifications = classifier(gen(noise)).detach()\n",
    "for i in range(grad_steps):\n",
    "    opt.zero_grad()\n",
    "    fake = gen(noise)\n",
    "    fake_image_history += [fake]\n",
    "    fake_score = get_score(\n",
    "        classifier(fake), \n",
    "        original_classifications,\n",
    "        target_indices,\n",
    "        other_indices,\n",
    "        penalty_weight=0.1\n",
    "    )\n",
    "    fake_score.backward()\n",
    "    noise.data = calculate_updated_noise(noise, 1 / grad_steps)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [n_images * 2, grad_steps * 2]\n",
    "show_tensor_images(torch.cat(fake_image_history[::skip], dim=2), num_images=n_images, nrow=n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode collapse\n",
    "\n",
    "When you are training multiple classes, there are multiple modes (multimodals) which the generator needs to be learned such as \"1\", \"2\", \"3\", and so on. If the generator has a task to create many classes from dataset, some classes are easier to generate. That will fool the discriminator easily. For example, in MNIST, number 1 can fool the discriminator, but number 3 cannot fool it. When the generator generates number 3, the discriminator will attack to the generator that it is fake but when the generator create number 1, the discriminator mostly say it is real. Thus, the generator will learn that \"OK, if I create '1', I will get reward\", then it will generate only number 1. That's is bias dataset creation called **Mode collapse**.\n",
    "\n",
    "During this whole training process, the discriminator naturally is trying to delineate this real and fake distribution as much as possible, whereas the generator is trying to make the generated distribution look more like the reals.\n",
    "However, let's take a step back again to the generator and discriminators roles. \n",
    "The discriminator needs to output just a single value prediction within zero and one.\n",
    "Whereas the generator actually needs to produce a pretty complex output composed of multiple features to try and fool the discriminator, for example, an image.\n",
    "As a result that discriminators job tends to be a little bit easier. That's why GANs may go to mode collapse. That is because the discriminator learns faster than the generator. Finally, the cost function from discriminato will go to nearly \"one\" then it will not tell anything to generator, it is called **vanishing gradients** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein loss (W-loss)\n",
    "\n",
    "To address the problem, we need to tune the learning of discriminator.\n",
    "\n",
    "Normally, The loss function of normal GANs or **BCE loss** is\n",
    "$$\\mathcal{L} = \\min_d \\max_g -[\\mathbb{E}(\\log(d(x))) + \\mathbb{E}(1-\\log(d(g(z))))]$$\n",
    "\n",
    "That is come out between 0 and 1.\n",
    "\n",
    "The wasserstein loss or W-Loss approximates the Earth Mover's Distance. That is look similar to the simplified form for the BCE loss. In this case the function calculates the difference between the expected values of the predictions of the discriminator, that is called the critic, and $c$ of a real example $x$, versus $c$ of a fake example $g$ of $z$. Generator taking in a noise vector to produce a fake image $g$ of $z$, or x-hat $\\hat{x}$.\n",
    "\n",
    "$$\\mathcal{L}_W = \\min_d \\max_c [\\mathbb{E}(c(x)) + \\mathbb{E}(c(g(z)))]$$\n",
    "\n",
    "The different of BCE loss and W-loss are BCE loss can be satuated from sigmoid function, but W-loss is the output of linear function. Thus, W-loss is come out any real number.\n",
    "\n",
    "<img src=\"img/WGANlosscompare.png\" title=\"Loss function\" style=\"width: 640px;\" />\n",
    "\n",
    "**Remark**: the right side should be **Critic output (W-GANs)** instead of *Discriminator output (W-GANs)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "def make_grad_hook():\n",
    "    '''\n",
    "    Function to keep track of gradients for visualization purposes, \n",
    "    which fills the grads list when using model.apply(grad_hook).\n",
    "    '''\n",
    "    grads = []\n",
    "    def grad_hook(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            grads.append(m.weight.grad)\n",
    "    return grads, grad_hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Generator and Noise, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(z_dim, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor,\n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, z_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "      n_samples: the number of samples to generate, a scalar\n",
    "      z_dim: the dimension of the noise vector, a scalar\n",
    "      device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Critic class\n",
    "\n",
    "Instead of create discriminator class, you must create critic class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    '''\n",
    "    Critic Class\n",
    "    Values:\n",
    "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_chan=1, hidden_dim=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.crit = nn.Sequential(\n",
    "            self.make_crit_block(im_chan, hidden_dim),\n",
    "            self.make_crit_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_crit_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_crit_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a critic block of DCGAN;\n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),  # Final Layer!!! difference from discriminator now\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the critic: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_chan)\n",
    "        '''\n",
    "        crit_pred = self.crit(image)\n",
    "        return crit_pred.view(len(crit_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Initializations\n",
    "Now you can start putting it all together.\n",
    "As usual, you will start by setting the parameters:\n",
    "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
    "  *   z_dim: the dimension of the noise vector\n",
    "  *   display_step: how often to display/visualize the images\n",
    "  *   batch_size: the number of images per forward/backward pass\n",
    "  *   lr: the learning rate\n",
    "  *   beta_1, beta_2: the momentum terms\n",
    "  *   c_lambda: weight of the gradient penalty\n",
    "  *   crit_repeats: number of times to update the critic per generator update - there are more details about this in the *Putting It All Together* section\n",
    "  *   device: the device type\n",
    "\n",
    "You will also load and transform the MNIST dataset to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "z_dim = 64\n",
    "display_step = 5000\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "c_lambda = 10\n",
    "crit_repeats = 5\n",
    "device = 'cuda:1'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST('.', download=False, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can initialize your generator, critic, and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "crit = Critic().to(device) \n",
    "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "crit = crit.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Penalty\n",
    "Calculating the gradient penalty can be broken into two functions: (1) compute the gradient with respect to the images and (2) compute the gradient penalty given the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(crit, real, fake, epsilon):\n",
    "    '''\n",
    "    Return the gradient of the critic's scores with respect to mixes of real and fake images.\n",
    "    Parameters:\n",
    "        crit: the critic model\n",
    "        real: a batch of real images\n",
    "        fake: a batch of fake images\n",
    "        epsilon: a vector of the uniformly random proportions of real/fake per mixed image\n",
    "    Returns:\n",
    "        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
    "    '''\n",
    "    # Mix the images together\n",
    "    mixed_images = real * epsilon + fake * (1 - epsilon)\n",
    "\n",
    "    # Calculate the critic's scores on the mixed images\n",
    "    mixed_scores = crit(mixed_images)\n",
    "    \n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        # Note: You need to take the gradient of outputs with respect to inputs.\n",
    "        # This documentation may be useful, but it should not be necessary:\n",
    "        # https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
    "        inputs = mixed_images,\n",
    "        outputs= mixed_scores,\n",
    "        # These other parameters have to do with the pytorch autograd engine works\n",
    "        grad_outputs=torch.ones_like(mixed_scores), \n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function you need to complete is to compute the gradient penalty given the gradient. First, you calculate the magnitude of each image's gradient. The magnitude of a gradient is also called the norm. Then, you calculate the penalty by squaring the distance between each magnitude and the ideal norm of 1 and taking the mean of all the squared distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(gradient):\n",
    "    '''\n",
    "    Return the gradient penalty, given a gradient.\n",
    "    Given a batch of image gradients, you calculate the magnitude of each image's gradient\n",
    "    and penalize the mean quadratic distance of each magnitude to 1.\n",
    "    Parameters:\n",
    "        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
    "    Returns:\n",
    "        penalty: the gradient penalty\n",
    "    '''\n",
    "    # Flatten the gradients so that each row captures one image\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "\n",
    "    # Calculate the magnitude of every row\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    \n",
    "    # Penalize the mean squared distance of the gradient norms from 1\n",
    "    penalty = torch.mean(gradient_norm - 1) ** 2\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses\n",
    "Next, you need to calculate the loss for the generator and the critic.\n",
    "\n",
    "For the generator, the loss is calculated by maximizing the critic's prediction on the generator's fake images. The argument has the scores for all fake images in the batch, but you will use the mean of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_loss(crit_fake_pred):\n",
    "    '''\n",
    "    Return the loss of a generator given the critic's scores of the generator's fake images.\n",
    "    Parameters:\n",
    "        crit_fake_pred: the critic's scores of the fake images\n",
    "    Returns:\n",
    "        gen_loss: a scalar loss value for the current batch of the generator\n",
    "    '''\n",
    "    gen_loss = -1 * torch.mean(crit_fake_pred)\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the critic, the loss is calculated by maximizing the distance between the critic's predictions on the real images and the predictions on the fake images while also adding a gradient penalty. The gradient penalty is weighed according to lambda. The arguments are the scores for all the images in the batch, and you will use the mean of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n",
    "    '''\n",
    "    Return the loss of a critic given the critic's scores for fake and real images,\n",
    "    the gradient penalty, and gradient penalty weight.\n",
    "    Parameters:\n",
    "        crit_fake_pred: the critic's scores of the fake images\n",
    "        crit_real_pred: the critic's scores of the real images\n",
    "        gp: the unweighted gradient penalty\n",
    "        c_lambda: the current weight of the gradient penalty \n",
    "    Returns:\n",
    "        crit_loss: a scalar for the critic's loss, accounting for the relevant factors\n",
    "    '''\n",
    "    crit_loss = (torch.mean(crit_fake_pred) - torch.mean(crit_real_pred)) + c_lambda * gp\n",
    "    return crit_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together!\n",
    "\n",
    "1.   Even on GPU, the **training will run more slowly** because the gradient penalty requires you to compute the gradient of a gradient -- this means potentially a few minutes per epoch! For best results, run this for as long as you can while on GPU.\n",
    "2.   One important difference from earlier versions is that you will **update the critic multiple times** every time you update the generator This helps prevent the generator from overpowering the critic. Sometimes, you might see the reverse, with the generator updated more times than the critic. This depends on architectural (e.g. the depth and width of the network) and algorithmic choices (e.g. which loss you're using). \n",
    "3.   WGAN-GP isn't necessarily meant to improve overall performance of a GAN, but just **increases stability** and avoids mode collapse. In general, a WGAN will be able to train in a much more stable way than the vanilla DCGAN from last assignment, though it will generally run a bit slower. You should also be able to train your model for more epochs without it collapsing.\n",
    "\n",
    "If you have time, train for 25,000 steps. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "critic_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "\n",
    "        mean_iteration_critic_loss = 0\n",
    "        for _ in range(crit_repeats):\n",
    "            ### Update critic ###\n",
    "            crit_opt.zero_grad()\n",
    "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "            fake = gen(fake_noise)\n",
    "            crit_fake_pred = crit(fake.detach())\n",
    "            crit_real_pred = crit(real)\n",
    "\n",
    "            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
    "            gradient = get_gradient(crit, real, fake.detach(), epsilon)\n",
    "            gp = gradient_penalty(gradient)\n",
    "            crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
    "\n",
    "            # Keep track of the average critic loss in this batch\n",
    "            mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n",
    "            # Update gradients\n",
    "            crit_loss.backward(retain_graph=True)\n",
    "            # Update optimizer\n",
    "            crit_opt.step()\n",
    "        critic_losses += [mean_iteration_critic_loss]\n",
    "\n",
    "        ### Update generator ###\n",
    "        gen_opt.zero_grad()\n",
    "        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        fake_2 = gen(fake_noise_2)\n",
    "        crit_fake_pred = crit(fake_2)\n",
    "        \n",
    "        gen_loss = get_gen_loss(crit_fake_pred)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        generator_losses += [gen_loss.item()]\n",
    "\n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            crit_mean = sum(critic_losses[-display_step:]) / display_step\n",
    "            print(f\"Step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n",
    "            show_tensor_images(fake)\n",
    "            show_tensor_images(real)\n",
    "            step_bins = 20\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Critic Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        cur_step += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Do the following on your own:\n",
    "1. Reproduce the vanilla GAN and DCGAN results on MNIST and CIFAR. Get the training and test loss for the generator and discriminator over time, plot them, and interpret them.\n",
    "2. Develop your own GAN to model data generated as follows:\n",
    "   $$ \\begin{eqnarray} \\theta & \\sim & {\\cal U}(0,2\\pi) \\\\\n",
    "                      r      & \\sim & {\\cal N}(0, 1) \\\\\n",
    "                      \\mathbf{x} & \\leftarrow & \\begin{cases} \\begin{bmatrix} (10+r)\\cos\\theta \\\\ (10+r)\\sin\\theta + 10\\end{bmatrix} & \\frac{1}{2}\\pi \\le \\theta \\le \\frac{3}{2}\\pi \\\\ \\begin{bmatrix} (10+r)\\cos\\theta \\\\ (10+r)\\sin\\theta - 10\\end{bmatrix} & \\mathrm{otherwise} \\end{cases} \\end{eqnarray} $$\n",
    "   You should create a PyTorch DataSet that generates the 2D data in the `__init__()` method, outputs a sample in the `__getitem__()` method, and returns the\n",
    "   dataset size in the `__len__()` method. Use the vanilla GAN approach above with an appropriate structure for the generator. Can your GAN generate a convincing\n",
    "   facsimile of a set of samples from the actual distribution?\n",
    "3. Use the DCGAN (or an improvement to it) to build a generator for a face image set of your choice. Can you get realistic faces that are not in the training set?\n",
    "\n",
    "As always, submit a brief report documenting your experiments and results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
